{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39c94b5-d17f-459e-88e2-91738e6f9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import xlrd\n",
    "import zipfile\n",
    "import requests\n",
    "import functools\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a4af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url):\n",
    "    \"\"\"\n",
    "    Request a HTTP GET method to the given url (for REST API)\n",
    "    and return its response as the dict object.\n",
    "\n",
    "    Args:\n",
    "    ====\n",
    "    url: string\n",
    "        valid url for REST API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"HTTP GET\", url)\n",
    "        r = requests.get(url)\n",
    "        json_dict = r.json()\n",
    "        return json_dict\n",
    "    except requests.exceptions.RequestException as error:    \n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d34555b2-4f22-43f9-a22e-55aa0b3e6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(url, filepath, enc=\"utf-8\", dec=\"utf-8\", logging=False):\n",
    "    \"\"\"\n",
    "    Request a HTTP GET method to the given url (for REST API)\n",
    "    and save its response as the csv file.\n",
    "\n",
    "    url: string\n",
    "        valid url for REST API\n",
    "    filepathe: string\n",
    "        valid path to the destination file\n",
    "    enc: string\n",
    "        encoding type for a content in a given url\n",
    "    dec: string\n",
    "        decoding type for a content in a downloaded file\n",
    "            dec = 'utf-8' for general env\n",
    "            dec = 'sjis'  for Excel on Win\n",
    "            dec = 'cp932' for Excel with extended JP str on Win\n",
    "    logging: True/False\n",
    "        flag whether putting process log\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if logging:\n",
    "            print(\"HTTP GET\", url)\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(filepath, 'w', encoding=enc) as f:\n",
    "            f.write(r.content.decode(dec))\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "def download_all_csv(\n",
    "        urls,\n",
    "        filepathes,\n",
    "        max_workers=10,\n",
    "        enc=\"utf-8\",\n",
    "        dec=\"utf-8\"):\n",
    "    \"\"\"\n",
    "    Request some HTTP GET methods to the given urls (for REST API)\n",
    "    and save each response as the csv file.\n",
    "    (!! This method uses multi threading when calling HTTP GET requests\n",
    "    and downloading files in order to improve the processing speed.)\n",
    "\n",
    "    urls: list of strings\n",
    "        valid urls for REST API\n",
    "    filepathes: list of strings\n",
    "        valid pathes to the destination file\n",
    "    max_workers: int\n",
    "        max number of working threads of CPUs within executing this method.\n",
    "    enc: string\n",
    "        encoding type for a content in a given url\n",
    "    dec: string\n",
    "        decoding type for a content in a downloaded file\n",
    "            dec = 'utf-8' for general env\n",
    "            dec = 'sjis'  for Excel on Win\n",
    "            dec = 'cp932' for Excel with extended JP str on Win\n",
    "    logging: True/False\n",
    "    \"\"\"\n",
    "    func = functools.partial(download_csv, enc=enc, dec=dec)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(\n",
    "            tqdm(executor.map(func, urls, filepathes), total=len(urls))\n",
    "        )\n",
    "        del results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ea7e52-9d25-41d8-846b-94744173d715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP GET https://api.e-stat.go.jp/rest/3.0/app/json/getStatsList?appId=d6c94977d11c6f2f1c3af613bc83a1c7a93941b1&lang=J&statsCode=00550020&searchKind=1&explanationGetFlg=N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:22<00:00,  8.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from estat_api import EstatRestAPI_URLParser\n",
    "estatapi_url_parser = EstatRestAPI_URLParser()  # URL Parser\n",
    "\n",
    "def search_tables():\n",
    "    \"\"\"\n",
    "    Prams (dictionary) to search eStat tables.\n",
    "    For more details, see also\n",
    "    https://www.e-stat.go.jp/api/api-info/e-stat-manual3-0#api_3_2\n",
    "\n",
    "        - appId: Application ID (*required)\n",
    "        - lang: 言語(J:日本語, E:英語)\n",
    "        - surveyYears: 調査年月 (YYYYY or YYYYMM or YYYYMM-YYYYMM)\n",
    "        - openYears: 調査年月と同様\n",
    "        - statsField: 統計分野 (2桁:統計大分類, 4桁:統計小分類)\n",
    "        - statsCode: 政府統計コード (8桁)\n",
    "        - searchWord: 検索キーワード\n",
    "        - searchKind: データの種別 (1:統計情報, 2:小地域・地域メッシュ)     \n",
    "        - collectArea: 集計地域区分 (1:全国, 2:都道府県, 3:市区町村)        \n",
    "        - explanationGetFlg: 解説情報有無(Y or N)\n",
    "        - ...\n",
    "    \"\"\"\n",
    "    params_dict = {\n",
    "        \"appId\": estatapi_url_parser.app_id,\n",
    "        \"lang\": \"J\",\n",
    "        \"statsCode\": \"00550020\",\n",
    "        # \"searchWord\": \"商業統計調査\",  # \"統計でみる市区町村のすがた\",\n",
    "        \"searchKind\": 1,\n",
    "        # \"collectArea\": 3,\n",
    "        \"explanationGetFlg\": \"N\"\n",
    "    }\n",
    "\n",
    "    url = estatapi_url_parser.getStatsListURL(params_dict, format=\"json\")   \n",
    "    json_dict = get_json(url)\n",
    "    # pprint(json_dict)\n",
    "\n",
    "    if json_dict['GET_STATS_LIST']['DATALIST_INF']['NUMBER'] != 0:\n",
    "        tables = json_dict[\"GET_STATS_LIST\"][\"DATALIST_INF\"][\"TABLE_INF\"]\n",
    "    else:\n",
    "        tables = []\n",
    "    return tables\n",
    "\n",
    "\n",
    "def parse_table_id(table):\n",
    "    return table[\"@id\"]\n",
    "\n",
    "\n",
    "def parse_table_raw_size(table):\n",
    "    return table[\"OVERALL_TOTAL_NUMBER\"]\n",
    "\n",
    "\n",
    "def parse_table_urls(table_id, table_raw_size, csv_raw_size=100000):\n",
    "    urls = []\n",
    "    for j in range(0, int(table_raw_size / csv_raw_size) + 1):\n",
    "        start_pos = j * csv_raw_size + 1\n",
    "        params_dict = {\n",
    "            \"appId\": estatapi_url_parser.app_id,  # Application ID\n",
    "            \"lang\": \"J\",  # 言語 (J: 日本語, E: 英語)\n",
    "            \"statsDataId\": str(table_id),  # 統計表ID\n",
    "            \"startPosition\": start_pos,  # 開始行\n",
    "            \"limit\": csv_raw_size,  # データ取得件数\n",
    "            \"explanationGetFlg\": \"N\",  # 解説情報有無(Y or N)\n",
    "            \"annotationGetFlg\": \"N\",  # 注釈情報有無(Y or N)\n",
    "            \"metaGetFlg\": \"N\",  # メタ情報有無(Y or N)\n",
    "            \"sectionHeaderFlg\": \"2\",  # CSVのヘッダフラグ(1:取得, 2:取得無)\n",
    "        }\n",
    "        url = estatapi_url_parser.getStatsDataURL(params_dict, format=\"csv\")\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CSV_RAW_SIZE = 100000\n",
    "\n",
    "    # list of tables\n",
    "    tables = search_tables()\n",
    "\n",
    "    # extract all table ids\n",
    "    if len(tables) == 0:\n",
    "        print(\"No tables were found.\")\n",
    "    elif len(tables) == 1:\n",
    "        table_ids = [parse_table_id(tables[0])]\n",
    "    else:\n",
    "        table_ids = list(map(parse_table_id, tables))\n",
    "\n",
    "    # list of urls\n",
    "    table_urls = []\n",
    "    table_raw_size = list(map(parse_table_raw_size, tables))\n",
    "    for i, table_id in enumerate(table_ids):\n",
    "        table_urls = table_urls + parse_table_urls(table_id, table_raw_size[i])\n",
    "\n",
    "    # list of filepathes\n",
    "    filepathes = []\n",
    "    for i, table_id in enumerate(table_ids):\n",
    "        table_name = tables[i][\"TITLE_SPEC\"][\"TABLE_NAME\"]\n",
    "        table_dir = f\"../data/{table_name}_{table_id}\"\n",
    "        os.makedirs(table_dir, exist_ok=True)\n",
    "        for j in range(0, int(table_raw_size[i] / CSV_RAW_SIZE) + 1):\n",
    "            filepath = f\"{table_dir}/{table_name}_{table_id}_{j}.csv\"\n",
    "            filepathes.append(filepath)\n",
    "\n",
    "    download_all_csv(table_urls, filepathes, max_workers=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131aaed8-7067-4154-a4bc-6b563f78a26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d20419c564893ff7827d87520a6c3b654e0969c6aa9ae3b46a277348f487f0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
